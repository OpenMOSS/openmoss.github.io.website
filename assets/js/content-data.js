(function () {
  window.SPA_DATA = {
    brand: {
      name: 'OpenMOSS',
      tagline: 'å¼€æ”¾ã€å¯ä¿¡èµ–çš„åŸºç¡€æ¨¡å‹ç ”ç©¶'
    },
    highlights: [
      {
        title: { zh: 'MOSS-Speech: çœŸè¯­éŸ³åˆ°è¯­éŸ³ç”Ÿæˆ', en: 'MOSS-Speech: True Speech-to-Speech Generation' },
        desc: { zh: 'åŸç”Ÿç«¯åˆ°ç«¯è¯­éŸ³äº¤äº’ï¼Œæ— éœ€ä»»ä½•ä¸­é—´æ–‡æœ¬å¼•å¯¼', en: 'Native end-to-end speech interaction without any intermediate text guidance' },
        date: '2025.10.1',
        url: 'https://arxiv.org/abs/2510.00499'
      },
      {
        title: { zh: 'XY-Tokenizer: ä½ç ç‡å£°å­¦è¯­ä¹‰ç»Ÿä¸€ç¼–ç ', en: 'XY-Tokenizer: Low-Bitrate Unified Acoustic-Semantic Encoding' },
        desc: { zh: '1kbpsæœ€å¼ºå£°å­¦è¯­ä¹‰ç»Ÿä¸€ç¼–ç åŠç¦»æ•£åŒ–å·¥å…·', en: 'State-of-the-art 1kbps unified acoustic-semantic encoding and discretization tool' },
        date: '2025.6.28',
        url: 'https://arxiv.org/abs/2506.23325'
      },
      {
        title: { zh: 'MOSS-TTSD: æ–‡æœ¬åˆ°å¯¹è¯è¯­éŸ³ç”Ÿæˆ', en: 'MOSS-TTSD: Text-to-Spoken Dialogue Generation' },
        desc: { zh: 'å¼€æºå¯¹è¯è¯­éŸ³ç”Ÿæˆæ¨¡å‹ï¼Œé«˜è¡¨ç°åŠ›ï¼Œå¤šè¯´è¯äººï¼Œè¶…é•¿è¯­éŸ³ç”Ÿæˆ', en: 'Open-source dialogue speech generation model with high expressiveness, multi-speaker, and long-form speech generation' },
        date: '2025.6.20',
        url: 'https://www.open-moss.com/en/moss-ttsd/'
      }
    ],
    courses: [
      { titleKey: 'resources.course.prml', descKey: 'resources.course.prml.desc', url: 'https://mooc1.chaoxing.com/course/224348208.html', labelKey: 'resources.btn.course' },
      { titleKey: 'resources.course.exercises', descKey: 'resources.course.exercises.desc', url: 'https://fudan-nlp.feishu.cn/wiki/WFifwXxfQiI3PKkn9FEcy0wKnjh', labelKey: 'resources.btn.exercise' },
      { titleKey: 'resources.course.community', descKey: 'resources.course.community.desc', url: 'https://github.com/WillQvQ/SummerQuest-2025', labelKey: 'resources.btn.summer' }
    ],
    projects: [
      { name: 'MOSS', descKey: 'resources.project.moss', stars: '12k+ â­', stack: 'Python', url: 'https://github.com/OpenMOSS/MOSS' },
      { name: 'AnyGPT', descKey: 'resources.project.anygpt', stars: '500+ â­', stack: 'Python', url: 'https://github.com/OpenMOSS/AnyGPT' },
      { name: 'MOSS-TTSD', descKey: 'resources.project.ttsd', stars: '200+ â­', stack: 'Python', url: 'https://github.com/OpenMOSS/MOSS-TTSD' },
      { name: 'SpeechGPT-2.0', descKey: 'resources.project.speechgpt', stars: '360+ â­', stack: 'Python', url: 'https://github.com/OpenMOSS/SpeechGPT-2.0-preview' },
      { name: 'DiRL', descKey: 'resources.project.dirl', stars: '100+ â­', stack: 'Python', url: 'https://github.com/OpenMOSS/DiRL' },
      { name: 'Language-Model-SAEs', descKey: 'resources.project.saes', stars: '164+ â­', stack: 'Python', url: 'https://github.com/OpenMOSS/Language-Model-SAEs' }
    ],
    positionCards: [
      { id: 'graduate', titleKey: 'positions.card.phd', descKey: 'positions.card.phd.desc' },
      { id: 'graduate', titleKey: 'positions.card.master', descKey: 'positions.card.master.desc' },
      { id: 'intern', titleKey: 'positions.card.intern', descKey: 'positions.card.intern.desc' },
      { id: 'postdoc', titleKey: 'positions.card.postdoc', descKey: 'positions.card.postdoc.desc' },
      { id: 'engineer', titleKey: 'positions.card.engineer', descKey: 'positions.card.engineer.desc' },
      { id: 'visiting', titleKey: 'positions.card.visiting', descKey: 'positions.card.visiting.desc' }
    ],
    whyUs: [
      { icon: 'âœ¨', titleKey: 'positions.why.research', descKey: 'positions.why.research.desc' },
      { icon: 'ğŸš€', titleKey: 'positions.why.resources', descKey: 'positions.why.resources.desc' },
      { icon: 'ğŸ‘¥', titleKey: 'positions.why.team', descKey: 'positions.why.team.desc' },
      { icon: 'ğŸ’¡', titleKey: 'positions.why.opensource', descKey: 'positions.why.opensource.desc' },
      { icon: 'ğŸŒ', titleKey: 'positions.why.collaboration', descKey: 'positions.why.collaboration.desc' },
      { icon: 'ğŸ“ˆ', titleKey: 'positions.why.career', descKey: 'positions.why.career.desc' }
    ],
    positions: {
      applyUrl: 'https://fudannlp.feishu.cn/share/base/form/shrcn29UYq1MCpTH0GBZh3AWPPg',
      // èŒä½è¯¦æƒ…ï¼ˆåŒè¯­æ”¯æŒï¼‰
      details: [
        {
          id: 'graduate',
          title: { zh: 'åšå£«/ç¡•å£«ç ”ç©¶ç”Ÿ', en: 'PhD/Master\'s Students' },
          blocks: [
            {
              subtitle: { zh: 'æ‹›æ”¶å¯¹è±¡', en: 'Target Candidates' },
              paragraphs: {
                zh: [
                  'æˆ‘ä»¬ä¸»è¦æ‹›æ”¶æœ‰å¿—äºä»äº‹å¤§æ¨¡å‹é¢†åŸŸç§‘å­¦ç ”ç©¶å’Œè½åœ°åº”ç”¨çš„å­¦ç”Ÿï¼Œå¹¶å¸Œæœ›æœ‰å¦‚ä¸‹ç‰¹ç‚¹ï¼šæ€ç»´æ´»è·ƒã€ç§¯æä¸»åŠ¨ã€çƒ­çˆ±ç ”ç©¶æˆ–å¼€å‘ã€åˆ»è‹¦å‹¤å¥‹ã€ä¸æ€•å¤±è´¥ã€‚å¦‚æœåªæ˜¯ä¸ºäº†æ··å­¦ä½æˆ–å¥½æ‰¾å·¥ä½œï¼Œè¯·å‹¿è”ç³»ï¼',
                  '2025å¹´ï¼Œæœ¬ç»„ä¸»è¦æ‹›ç”Ÿæ–¹å‘ä¸ºå¤§æ¨¡å‹é¢„è®­ç»ƒã€AI Infraã€æ–°æ¶æ„ã€å¤šæ¨¡æ€èåˆã€æ™ºèƒ½ä½“ã€å…·èº«æ™ºèƒ½ï¼Œæœ‰å…´è¶£å­¦ç”Ÿæ¬¢è¿è”ç³»ã€‚'
                ],
                en: [
                  'We primarily recruit students committed to research and applications in large language models, with the following qualities: active thinking, proactive attitude, passion for research or development, diligence, and resilience. If you are only seeking a degree or job prospects, please do not apply.',
                  'In 2025, our main research areas include LLM pre-training, AI Infrastructure, novel architectures, multimodal fusion, agents, and embodied intelligence. Interested students are welcome to contact us.'
                ]
              }
            },
            {
              subtitle: { zh: 'é‡è¦äº‹é¡¹', en: 'Important Notes' },
              paragraphs: {
                zh: [
                  'é™¤é¡¹ç›®åˆä½œæ¨èå®ä¹ å¤–ï¼Œæœ¬ç»„ä¸å…è®¸å­¦ç”Ÿåœ¨ç ”ä¸‰ã€åšäº”ä¹‹å‰è¿›è¡Œä»»ä½•å½¢å¼çš„å®ä¹ ã€‚',
                  'æœ¬ç»„ç ”ç©¶ç”Ÿéƒ½ä¼šæ ¹æ®éœ€è¦å®‰æ’ä¸€å®šçš„å·¥ç¨‹ä»»åŠ¡ã€‚å¦‚æœä¸æ„¿æ„å‚ä¸å·¥ç¨‹é¡¹ç›®çš„ï¼Œä¹Ÿä¸è¦é€‰æ‹©æœ¬ç»„ã€‚'
                ],
                en: [
                  'Except for project-related internships, students are not allowed to take any internships before their third year of master\'s or fifth year of PhD studies.',
                  'All graduate students will be assigned engineering tasks as needed. If you are unwilling to participate in engineering projects, please do not choose our group.'
                ]
              }
            },
            {
              subtitle: { zh: 'æ‹›ç”Ÿè¯´æ˜', en: 'Admission Information' },
              paragraphs: {
                zh: [
                  'ï¼ˆ1ï¼‰ç¡•å£«åé¢ï¼šè§†ç ”ç©¶ç»è´¹è€Œå®šï¼Œè¿‘ä¸‰å¹´å†…æ¯å¹´å­¦ç¡•1åã€ä¿ç ”ä¸“ç¡•1-2åã€è€ƒç ”ä¸“ç¡•çº¦10åã€‚æ—¥å¸¸ç§‘ç ”ä¸­æˆ‘ä»¬ä¸åŒºåˆ†å­¦ç¡•å’Œä¸“ç¡•ï¼Œå¹¶ä¸”éƒ½æœ‰èµ„æ ¼è½¬åšã€‚',
                  'ï¼ˆ2ï¼‰åšå£«åé¢ï¼šè§†ç ”ç©¶ç»è´¹è€Œå®šï¼Œè¿‘ä¸‰å¹´å†…æ¯å¹´åšå£«åé¢2-3åã€‚æœ¬äººåœ¨ä¸Šæµ·åˆ›æ™ºå­¦é™¢æ‹›ç›´åšç”Ÿï¼Œå’Œå¤æ—¦è”åŸ¹ï¼Œåé¢ä¸é™ã€‚å»ºè®®å…ˆæ‹¿åˆ°ä¸Šæµ·åˆ›æ™ºå­¦é™¢ offer å†æŠ¥åå¤æ—¦å¤ä»¤è¥ã€‚',
                  'ï¼ˆ3ï¼‰æ‹›ç”Ÿé€”å¾„ï¼šå¤ä»¤è¥ã€ç§‹å­£ä¿ç ”ã€è€ƒç ”é¢è¯•å‡éœ€é€šè¿‡å­¦é™¢åˆå®¡åè”ç³»ï¼Œæˆ‘ä»¬ä¼šå®‰æ’ç»„å†…é¢è¯•ã€‚',
                  'ï¼ˆ4ï¼‰æœ¬ç»„é¢è¯•ï¼šé‡è§†ç§‘ç ”æ½œè´¨ã€å·¥ç¨‹èƒ½åŠ›å’Œå›¢é˜Ÿåä½œã€‚é«˜è´¨é‡å®Œæˆ nlp-beginner çš„å­¦ç”Ÿä¼˜å…ˆã€‚',
                  'ï¼ˆ5ï¼‰æå‰è¿›ç»„ï¼šå¸Œæœ›ç¡®å®šæ¥æœ¬ç»„çš„åŒå­¦æå‰è¿›ç»„å­¦ä¹ ï¼Œè¡¨ç°ä¼˜ç§€å¯æ¨èå­—èŠ‚ã€åä¸ºç­‰åˆä½œå•ä½å®ä¹ ã€‚',
                  'ï¼ˆ6ï¼‰ç ”ç©¶ç”Ÿå¾…é‡ï¼šåœ¨å­¦æ ¡è¡¥è´´åŸºç¡€ä¸Šæä¾›æœ‰ç«äº‰åŠ›çš„è¡¥åŠ©ï¼Œå¹¶å¯¹ä¸“ç¡•ç»™äºˆä¸€å®šä½æˆ¿è¡¥è´´ã€‚'
                ],
                en: [
                  '(1) Master\'s positions: Subject to research funding, approximately 1 academic master, 1-2 professional masters (recommendation), and ~10 professional masters (entrance exam) per year. We do not distinguish between academic and professional masters in research, and all are eligible for PhD conversion.',
                  '(2) PhD positions: Subject to research funding, 2-3 positions per year. Direct PhD positions are available at Shanghai Innovation Institute in collaboration with Fudan, with no limit on positions. We recommend securing an offer from Shanghai Innovation Institute before applying to Fudan\'s summer camp.',
                  '(3) Admission channels: Summer camp, autumn recommendation, and entrance exam interviews all require passing the school\'s preliminary review before contacting us. We will arrange group interviews.',
                  '(4) Group interview: We value research potential, engineering capabilities, and teamwork. Students who complete nlp-beginner with high quality are prioritized.',
                  '(5) Early joining: Students who are certain about joining our group are encouraged to join early. Outstanding performers can be recommended for internships at partner companies like ByteDance and Huawei.',
                  '(6) Graduate benefits: Competitive stipends are provided in addition to university subsidies, with housing allowances for professional masters.'
                ]
              }
            }
          ]
        },
        {
          id: 'intern',
          title: { zh: 'å®ä¹ ç”Ÿ', en: 'Interns' },
          blocks: [
            {
              subtitle: { zh: 'èŒä½ä»‹ç»', en: 'Position Description' },
              paragraphs: {
                zh: ['è¯·é€šè¿‡é‚®ä»¶ï¼ˆllm@fudan.edu.cnï¼‰æˆ– <a href="https://fudannlp.feishu.cn/share/base/form/shrcn29UYq1MCpTH0GBZh3AWPPg" target="_blank" style="color: var(--fudan-blue); text-decoration: underline;">é—®å·</a> è¿›è¡ŒæŠ•é€’ï¼Œå¯¹äºéæœ¬åœ°å­¦ç”Ÿæœ‰ 2000/æœˆçš„ä½æˆ¿è¡¥è´´ã€‚'],
                en: ['Please apply via email (llm@fudan.edu.cn) or <a href="https://fudannlp.feishu.cn/share/base/form/shrcn29UYq1MCpTH0GBZh3AWPPg" target="_blank" style="color: var(--fudan-blue); text-decoration: underline;">application form</a>. Non-local students receive a housing allowance of 2000 CNY/month.']
              }
            }
          ]
        },
        {
          id: 'postdoc',
          title: { zh: 'åšå£«åç ”ç©¶å‘˜', en: 'Postdoctoral Researchers' },
          blocks: [
            {
              subtitle: { zh: 'ç ”ç©¶æ–¹å‘', en: 'Research Areas' },
              paragraphs: {
                zh: ['AI Infra', 'å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ', 'å¤šæ¨¡æ€å¤§æ¨¡å‹', 'è¯­éŸ³å¤§æ¨¡å‹', 'å…·èº«æ™ºèƒ½'],
                en: ['AI Infrastructure', 'LLM Pre-training', 'Multimodal Models', 'Speech Models', 'Embodied Intelligence']
              }
            },
            {
              subtitle: { zh: 'ç”³è¯·äººè¦æ±‚', en: 'Requirements' },
              paragraphs: {
                zh: [
                  'åšå£«æœŸé—´ä¸“ä¸šæ–¹å‘ä¸ºè®¡ç®—æœºã€è½¯ä»¶ã€ç”µå­ã€è‡ªåŠ¨åŒ–ã€æ•°å­¦ç­‰ç›¸å…³æ–¹å‘ã€‚',
                  'å…·å¤‡å›½å†…å¤–ä¼˜ç§€å¤§å­¦åšå£«å­¦ä½ï¼Œæ¯•ä¸šä¸è¶…è¿‡ 3 å¹´ã€‚',
                  'å¹´é¾„åœ¨ 35 å‘¨å²ä»¥ä¸‹ã€‚',
                  'åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå‘è¡¨é«˜æ°´å¹³æ–‡ç« æˆ–ä¸»æŒ/å‚ä¸å®é™…é¡¹ç›®è€…ä¼˜å…ˆã€‚',
                  'æ€ç»´æ´»è·ƒã€åˆ›æ–°èƒ½åŠ›å¼ºï¼Œå¯¹ç ”å‘å……æ»¡çƒ­æƒ…ï¼Œè´£ä»»å¿ƒå¼ºã€‚'
                ],
                en: [
                  'PhD in Computer Science, Software Engineering, Electronics, Automation, Mathematics, or related fields.',
                  'PhD degree from a reputable university, graduated within 3 years.',
                  'Under 35 years of age.',
                  'Preference for candidates with high-quality publications in AI or experience leading/participating in real projects.',
                  'Active thinking, strong innovation capability, passionate about R&D, and highly responsible.'
                ]
              }
            },
            {
              subtitle: { zh: 'å·¥èµ„å¾…é‡', en: 'Compensation' },
              paragraphs: {
                zh: [
                  'æŒ‰ç…§å¤æ—¦å¤§å­¦å¯¹åšå£«åçš„ç›¸å…³è§„å®šæä¾›å¾…é‡ï¼Œäº«å—å…¬å¯“ä¸ç¦åˆ©ï¼Œå¯ç”³è¯·å›½å®¶"åšæ–°è®¡åˆ’""å¼•è¿›è®¡åˆ’"åŠä¸Šæµ·ã€å¸‚çº§è¶…çº§åšå£«åé¡¹ç›®ã€‚',
                  'è¯¾é¢˜ç»„æ ¹æ®ä¸ªäººç ”ç©¶è¿›å±•ç»™äºˆé¢å¤–è¡¥è´´ï¼Œå¹¶æä¾›ä¼˜è¶Šç§‘ç ”æ¡ä»¶ã€‚'
                ],
                en: [
                  'Compensation follows Fudan University\'s postdoctoral regulations, with apartment and benefits. Eligible to apply for national programs such as "Postdoctoral Innovation Talent Support Program" and Shanghai/municipal super postdoctoral projects.',
                  'Additional stipends based on individual research progress, with excellent research conditions provided by the group.'
                ]
              }
            },
            {
              subtitle: { zh: 'ç”³è¯·æ–¹å¼', en: 'How to Apply' },
              paragraphs: {
                zh: [
                  'ç”³è¯·é‚®ä»¶å‘é€è‡³ llm@fudan.edu.cnï¼Œä¸»é¢˜æ³¨æ˜"åº”è˜åšå£«å - å§“å - ä¸“ä¸š - å­¦æ ¡"ã€‚',
                  'é‚®ä»¶é™„ä¸ªäººç®€å†å¹¶è¯´æ˜æ„Ÿå…´è¶£çš„ç ”ç©¶æ–¹å‘ï¼Œåˆå®¡åæˆ‘ä»¬å°†ä¸æ‚¨è”ç³»ã€‚'
                ],
                en: [
                  'Send application email to llm@fudan.edu.cn with subject line "Postdoc Application - Name - Major - University".',
                  'Attach your CV and specify your research interests. We will contact you after preliminary review.'
                ]
              }
            }
          ]
        },
        {
          id: 'engineer',
          title: { zh: 'ç ”ç©¶å·¥ç¨‹å¸ˆ', en: 'Research Engineers' },
          blocks: [
            {
              subtitle: { zh: 'å…³äºå²—ä½', en: 'About the Position' },
              paragraphs: {
                zh: ['OpenMOSS å›¢é˜Ÿå› ç§‘ç ”å·¥ä½œéœ€è¦ï¼Œé•¿æœŸæ‹›è˜ç§‘ç ”å·¥ç¨‹åŠ©ç†ï¼Œå¾…é‡é¢è®®ã€‚'],
                en: ['OpenMOSS Team is recruiting research engineering assistants for long-term positions. Compensation is negotiable.']
              }
            },
            {
              subtitle: { zh: 'æ‹›è˜è¯´æ˜', en: 'Job Description' },
              paragraphs: {
                zh: [
                  'å‚ä¸å®éªŒå®¤çš„å¤§è¯­è¨€æ¨¡å‹å·¥ç¨‹å¼€å‘é¡¹ç›®ã€‚',
                  'å…·å¤‡è‰¯å¥½çš„å·¥ç¨‹ç»éªŒï¼Œç†Ÿç»ƒæŒæ¡ Pythonï¼Œç†Ÿæ‚‰ PyTorch å¹¶æœ‰ NLP é¡¹ç›®ç»éªŒè€…ä¼˜å…ˆã€‚',
                  'å…·æœ‰ä¸“ç ”ç²¾ç¥ï¼Œå·¥ä½œè¸å®è®¤çœŸã€‚'
                ],
                en: [
                  'Participate in the lab\'s LLM engineering and development projects.',
                  'Good engineering experience required. Proficiency in Python, familiarity with PyTorch, and NLP project experience preferred.',
                  'Dedicated, diligent, and responsible work attitude.'
                ]
              }
            },
            {
              subtitle: { zh: 'ç”³è¯·æ–¹å¼', en: 'How to Apply' },
              paragraphs: {
                zh: [
                  'ç”³è¯·é‚®ä»¶å‘é€è‡³ llm@fudan.edu.cnï¼Œä¸»é¢˜æ³¨æ˜"åº”è˜ç§‘ç ”å·¥ç¨‹åŠ©ç† - å§“å"ã€‚',
                  'é‚®ä»¶ä¸­é™„ç®€å†ï¼Œåˆå®¡åæˆ‘ä»¬å°†ä¸æ‚¨è”ç³»ã€‚'
                ],
                en: [
                  'Send application email to llm@fudan.edu.cn with subject line "Research Engineer Application - Name".',
                  'Attach your CV. We will contact you after preliminary review.'
                ]
              }
            }
          ]
        },
        {
          id: 'visiting',
          title: { zh: 'è®¿é—®å­¦è€…', en: 'Visiting Scholars' },
          blocks: [
            {
              subtitle: { zh: 'è¯´æ˜', en: 'Note' },
              paragraphs: {
                zh: ['è¯·é‚®ä»¶ xpqiu@fudan.edu.cn å’¨è¯¢ã€‚'],
                en: ['Please contact xpqiu@fudan.edu.cn for inquiries.']
              }
            }
          ]
        }
      ]
    },
    webmaster: {
      members: [
        { name: 'éƒ‘é€¸å®', role: 'ç½‘é¡µè®¾è®¡å¸ˆ' },
        { name: 'è´ºå¿ƒå˜‰', role: 'ç½‘é¡µè®¾è®¡å¸ˆ' }
      ]
    },
    footer: {
      contactLinks: [
        { label: 'GitHub', icon: 'fa-brands fa-github', url: 'https://github.com/OpenMOSS' },
        { label: 'Twitter', icon: 'fa-brands fa-x-twitter', url: 'https://x.com/Open_MOSS' },
        { label: 'Email', icon: 'fa-solid fa-envelope', url: 'mailto:llm@fudan.edu.cn' }
      ]
    },
    publications: {
      infra: [
        {
          title: 'SpeechGPT 2.0-preview: A GPT-4o-level Real-Time Spoken Dialogue System',
          authors: 'Hanfu Chen, Ke Chen, Qinyuan Cheng, Mingshu Chen, Ruifan Deng, Liwei Fan, Zhaoye Fei, QingHui Gao, Yitian Gong, Ching Wing Kwok, Kexin Huang, Yaozhou Jiang, Xingyu Lu, Shimin Li, Zhengyuan Lin, Ruixiao Li, Qian Tu, Jin Wang, Yang Wang, Siyin Wang, Zhe Xu, Chenchen Yang, Donghua Yu, Yuqian Yao, Yucheng Yuan, Chufan Yu, Dong Zhang, YiWei Zhao, Yuqian Zhang, Jun Zhan, Xin Zhang, Xingjian Zhao, Chengyang Zhu',
          venue: '',
          year: '2025',
          support: true,
          alphabetical: true,
          links: [
            { type: 'GitHub', url: 'https://github.com/OpenMOSS/SpeechGPT-2.0-preview' },
            { type: 'Blog', url: 'https://www.open-moss.com/en/speechgpt2-preview/' }
          ]
        },
        {
          title: 'MOSS-TTSD: Zero-Shot Multi-Speaker Dialogue Speech Synthesis',
          authors: 'Cheng Chang, Ke Chen, Mingshu Chen, Qinyuan Cheng, Ruifan Deng, Liwei Fan, Zhaoye Fei, Qinghui Gao, Yitian Gong, Kexin Huang, Botian Jiang, Yaozhou Jiang, Luozhijie Jin, Ruixiao Li, Shimin Li, Zhengyuan Lin, Xipeng Qiu, Qian Tu, Jin Wang, Ruiming Wang, Wenxuan Wang, Yang Wang, Chenchen Yang, Zhe Xu, Yucheng Yuan, Donghua Yu, Jun Zhan, Dong Zhang, Wenbo Zhang, Xin Zhang, Yuqian Zhang, Yiwei Zhao, Xingjian Zhao',
          venue: '',
          year: '2025',
          support: true,
          alphabetical: true,
          links: [
            { type: 'GitHub', url: 'https://github.com/OpenMOSS/MOSS-TTSD' },
            { type: 'Blog', url: 'https://www.open-moss.com/en/moss-ttsd/' }
          ]
        },
        {
          title: 'MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance',
          authors: 'Hanfu Chen, Ke Chen, Mingshu Chen, Qinyuan Cheng, Zhaoye Fei, Qinghui Gao, Yang Gao, Yitian Gong, Xuanjing Huang, Yaozhou Jiang, Luozhijie Jin, Ruixiao Li, Xipeng Qiu, Ruiming Wang, Yang Wang, Yuanfan Xu, Xiaogui Yang, Zhe Xu, Donghua Yu, Wenbo Zhang, Yiyang Zhang, Xingjian Zhao, Yaqian Zhou',
          venue: '',
          year: '2025',
          support: true,
          alphabetical: true,
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2510.00499' }]
        },
        {
          title: 'Towards Economical Inference: Enabling DeepSeek\'s Multi-Head Latent Attention in Any Transformer-based LLMs',
          authors: 'Tao Ji, Bin Guo, Yuanbin Wu, Qipeng Guo, Lixing Shen, Zhan Chen, Xipeng Qiu, Qi Zhang, Tao Gui',
          venue: 'ACL',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2502.14837' }]
        }
      ],
      multimodal: [
        {
          title: 'SpeechGPT 2.0-preview: A GPT-4o-level Real-Time Spoken Dialogue System',
          authors: 'Hanfu Chen, Ke Chen, Qinyuan Cheng, Mingshu Chen, Ruifan Deng, Liwei Fan, Zhaoye Fei, QingHui Gao, Yitian Gong, Ching Wing Kwok, Kexin Huang, Yaozhou Jiang, Xingyu Lu, Shimin Li, Zhengyuan Lin, Ruixiao Li, Qian Tu, Jin Wang, Yang Wang, Siyin Wang, Zhe Xu, Chenchen Yang, Donghua Yu, Yuqian Yao, Yucheng Yuan, Chufan Yu, Dong Zhang, YiWei Zhao, Yuqian Zhang, Jun Zhan, Xin Zhang, Xingjian Zhao, Chengyang Zhu',
          venue: '',
          year: '2025',
          alphabetical: true,
          links: [
            { type: 'GitHub', url: 'https://github.com/OpenMOSS/SpeechGPT-2.0-preview' },
            { type: 'Blog', url: 'https://www.open-moss.com/en/speechgpt2-preview/' }
          ]
        },
        {
          title: 'MOSS-TTSD: Zero-Shot Multi-Speaker Dialogue Speech Synthesis',
          authors: 'Cheng Chang, Ke Chen, Mingshu Chen, Qinyuan Cheng, Ruifan Deng, Liwei Fan, Zhaoye Fei, Qinghui Gao, Yitian Gong, Kexin Huang, Botian Jiang, Yaozhou Jiang, Luozhijie Jin, Ruixiao Li, Shimin Li, Zhengyuan Lin, Xipeng Qiu, Qian Tu, Jin Wang, Ruiming Wang, Wenxuan Wang, Yang Wang, Chenchen Yang, Zhe Xu, Yucheng Yuan, Donghua Yu, Jun Zhan, Dong Zhang, Wenbo Zhang, Xin Zhang, Yuqian Zhang, Yiwei Zhao, Xingjian Zhao',
          venue: '',
          year: '2025',
          alphabetical: true,
          links: [
            { type: 'GitHub', url: 'https://github.com/OpenMOSS/MOSS-TTSD' },
            { type: 'Blog', url: 'https://www.open-moss.com/en/moss-ttsd/' }
          ]
        },
        {
          title: 'MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance',
          authors: 'Hanfu Chen, Ke Chen, Mingshu Chen, Qinyuan Cheng, Zhaoye Fei, Qinghui Gao, Yang Gao, Yitian Gong, Xuanjing Huang, Yaozhou Jiang, Luozhijie Jin, Ruixiao Li, Xipeng Qiu, Ruiming Wang, Yang Wang, Yuanfan Xu, Xiaogui Yang, Zhe Xu, Donghua Yu, Wenbo Zhang, Yiyang Zhang, Xingjian Zhao, Yaqian Zhou',
          venue: '',
          year: '2025',
          alphabetical: true,
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2510.00499' }]
        },
        {
          title: 'InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems',
          authors: 'Kexin Huang, Qian Tu, Liwei Fan, Chenchen Yang, Dong Zhang, Shimin Li, Zhaoye Fei, Qinyuan Cheng, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.16381' }]
        },
        {
          title: 'XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs',
          authors: 'Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.23325' }]
        },
        {
          title: 'CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs',
          authors: 'Jinlan Fu, Shenzhen Huangfu, Hao Fei, Xiaoyu Shen, Bryan Hooi, Xipeng Qiu, See-Kiong Ng',
          venue: 'ICLR',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2501.16629' }]
        },
        {
          title: 'Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Models',
          authors: 'Siyin Wang, Xingsong Ye, Qinyuan Cheng, Junwen Duan, Shimin Li, Jinlan Fu, Xipeng Qiu, Xuanjing Huang',
          venue: 'NAACL',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2406.15279' }]
        },
        {
          title: 'VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search',
          authors: 'Yikun Wang, Siyin Wang, Qinyuan Cheng, Zhaoye Fei, Liang Ding, Qipeng Guo, D. Tao, Xipeng Qiu',
          venue: 'ACL',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2504.09130' }]
        },
        {
          title: 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context',
          authors: 'Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yu-Gang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2510.23763' }]
        }
      ],
      reasoning: [
        {
          title: 'Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections',
          authors: 'Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, Xipeng Qiu',
          venue: 'NeurIPS',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2507.00018' }]
        },
        {
          title: 'RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization',
          authors: 'Zeng Zhiyuan, Jiashuo Liu, Zhangyue Yin, Ge Zhang, Wenhao Huang, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2511.04285' }]
        },
        {
          title: 'Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective',
          authors: 'Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2412.14135' }]
        },
        {
          title: 'In-Memory Learning: A Declarative Learning Framework for Large Language Models',
          authors: 'Bo Wang, Tianxiang Sun, Hang Yan, Siyin Wang, Qingyuan Cheng, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2403.02757' }]
        },
        {
          title: 'Game-RL: Synthesizing Multimodal Verifiable Game Data to Boost VLMs\' General Reasoning',
          authors: 'Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Zhiheng Xi, Changhao Jiang, Zhangyue Yin, Yining Zheng, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2505.13886' }]
        }
      ],
      embodied: [
        {
          title: 'World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning',
          authors: 'Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, Xipeng Qiu',
          venue: 'ACL',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2503.10480' }]
        },
        {
          title: 'LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models',
          authors: 'Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2510.13626' }]
        },
        {
          title: 'VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks',
          authors: 'Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu',
          venue: 'ICCV',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2412.18194' }]
        },
        {
          title: 'World-aware Planning Narratives Enhance Large Vision-Language Model Planner',
          authors: 'Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, Xipeng Qiu',
          venue: 'NeurIPS',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.21230' }]
        },
        {
          title: 'Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning',
          authors: 'Zhaoye Fei, Li Ji, Siyin Wang, Junhao Shi, Jingjing Gong, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.23127' }]
        }
      ],
      safety: [
        {
          title: 'Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT',
          authors: 'Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2402.12201' }]
        },
        {
          title: 'Automatically Identifying Local and Global Circuits with Linear Computation Graphs',
          authors: 'Xuyang Ge, Fukang Zhu, Wentao Shu, Junxuan Wang, Zhengfu He, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2405.13868' }]
        },
        {
          title: 'Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures',
          authors: 'Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu',
          venue: 'ICLR',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2410.06672' }]
        },
        {
          title: 'Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders',
          authors: 'Zhengfu He, Wentao Shu, Xuyang Ge, Lingjie Chen, Junxuan Wang, Yunhua Zhou, Frances Liu, Qipeng Guo, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2410.20526' }]
        },
        {
          title: 'Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition',
          authors: 'Zhengfu He, Junxuan Wang, Rui Lin, Xuyang Ge, Wentao Shu, Qiong Tang, Junping Zhang, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2504.20938' }]
        },
        {
          title: 'Attention Layers Add Into Low-Dimensional Residual Subspaces',
          authors: 'Junxuan Wang, Xuyang Ge, Wentao Shu, Zhengfu He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2508.16929' }]
        },
        {
          title: 'Evolution of Concepts in Language Model Pre-Training',
          authors: 'Xuyang Ge, Wentao Shu, Jiaxing Wu, Yunhua Zhou, Zhengfu He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2509.17196' }]
        }
      ],
      arch: [
        {
          title: 'DiRL: an Efficient Training Framework for Diffusion Language Models',
          authors: 'Ying Zhu, Jiaxin Wan, Tianyi Liang, Xu Guo, Xiaoran Liu, Zengfeng Huang, Ziwei He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'GitHub', url: 'https://github.com/OpenMOSS/DiRL' }]
        },
        {
          title: 'Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction',
          authors: 'Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2508.02558' }]
        },
        {
          title: 'LONGLLADA: Unlocking Long Context Capabilities in Diffusion LLMs',
          authors: 'Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.14429' }]
        },
        {
          title: 'Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache',
          authors: 'Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.11886' }]
        },
        {
          title: 'Thus Spake Long-Context Large Language Model',
          authors: 'Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xuanjing Huang, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2502.17129' }]
        }
      ]
    }
  };
})();
